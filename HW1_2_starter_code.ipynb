{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.882 HW 1.2 Starter Code\n",
    "\n",
    "See the problem set handout for instructions and deliverables.\n",
    "\n",
    "See HW1.1 Starter Code for dependency installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bf-wPeAaeaRf",
    "outputId": "5eee15f2-567e-4464-bdcd-28298daffa31"
   },
   "outputs": [],
   "source": [
    "# Install dependencies (run this once ever 12 hours)\n",
    "!pip install --upgrade git+https://github.com/tomsilver/pddlgym # Install most recent PDDLGym (must be from source!)\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque\n",
    "from itertools import count, product\n",
    "from tabulate import tabulate\n",
    "import abc\n",
    "import copy\n",
    "import numpy as np\n",
    "import heapq as hq\n",
    "import pddlgym\n",
    "from pddlgym.structs import Predicate, State, Type, LiteralConjunction\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes\n",
    "First we define some convenient abstract classes for Approach, Planner, Heuristic, and Featurizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Approach:\n",
    "    \"\"\"Generic approach for learning and behaving in a domain.\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def set_actions(self, actions):\n",
    "        \"\"\"Tell the approach what actions are available in the domain\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        actions : [ Any ]\n",
    "            For a continuous action space, this would not work! If you are\n",
    "            curious how one might handle actions more generally, see\n",
    "            https://gym.openai.com/docs/#spaces.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def reset(self, state):\n",
    "        \"\"\"Tell the approach to prepare to take actions from the given initial state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : pddlgym.State\n",
    "            Note that the state contains the goal (state.goal).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        info : dict\n",
    "            Any logging or debugging info can go here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def step(self, state):\n",
    "        \"\"\"Ask the approach for an action to take given the input state.\n",
    "        Assume that the action will be subsequently executed in the environment.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : pddlgym.State\n",
    "            Note that the state contains the goal (state.goal).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        action : Any\n",
    "        info : dict\n",
    "            Any logging or debugging info can go here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def seed(self, seed):\n",
    "        \"\"\"Optionally set a random seed\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def train(self, env):\n",
    "        \"\"\"Some approaches learn. Others will do nothing for training.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env : pddlgym.PDDLEnv\n",
    "            A training environment that encapsulates training problems.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "        \n",
    "\n",
    "class Planner:\n",
    "    \"\"\"Generic class for planning\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, state):\n",
    "        \"\"\"Make a plan given the state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : pddlgym.State\n",
    "            Note that the state contains the goal (state.goal).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        actions : [ Any ]\n",
    "            The plan\n",
    "        info : dict\n",
    "            Any logging or debugging info can go here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def set_actions(self, actions):\n",
    "        \"\"\"Tell the planner what actions are available in the domain\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        actions : [ Any ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "        \n",
    "\n",
    "class Heuristic:\n",
    "    \"\"\"Generic class for heuristics\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, node):\n",
    "        \"\"\"Return a heuristic value (estimated cost-to-go) given a search node.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : AStar.Node\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        heuristic : float\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def set_actions(self, actions):\n",
    "        \"\"\"Tell the planner what actions are available in the domain\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        actions : [ Any ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train(self, env):\n",
    "        \"\"\"Some heuristics are learnable. Others will do nothing for training.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env : pddlgym.PDDLEnv\n",
    "            A training environment that encapsulates training problems.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "\n",
    "        \n",
    "class Featurizer:\n",
    "    \"\"\"Generic class for featurizers\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def initialize(self, all_data):\n",
    "        \"\"\"Initialize the featurizer from a training dataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        all_data : [ Any ]\n",
    "            A list of data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def apply(self, x):\n",
    "        \"\"\"Convert a raw input to a featurized input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Any\n",
    "            A raw input\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        xhat : Any\n",
    "            A featurized input\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def invert(self, xhat):\n",
    "        \"\"\"Convert a featurized input to a raw input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Any\n",
    "            A featurized input\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        x : Any\n",
    "            A raw input\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-learning approaches\n",
    "Now we define some non-learning approaches: Random, AStar, and Greedy Best-First Search. These approaches do not need to make use of featurizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomActions(Approach):\n",
    "    \"\"\"Take random actions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._actions = None\n",
    "        self._rng = None\n",
    "\n",
    "    def set_actions(self, actions):\n",
    "        self._actions = actions\n",
    "\n",
    "    def reset(self, state):\n",
    "        return {}\n",
    "\n",
    "    def step(self, state):\n",
    "        return self._rng.choice(self._actions)\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        \n",
    "    def train(self, env):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchApproach(Approach):\n",
    "    \"\"\"Make a plan and follow it\n",
    "    \"\"\"\n",
    "    def __init__(self, planner, heuristic=None):\n",
    "        self._planner = planner\n",
    "        self._heuristic = heuristic\n",
    "        self._actions = None\n",
    "        self._plan = []\n",
    "        self._rng = None\n",
    "\n",
    "    def set_actions(self, actions):\n",
    "        self._actions = actions\n",
    "        self._planner.set_actions(actions)\n",
    "\n",
    "    def reset(self, obs):\n",
    "        self._plan, info = self._planner(obs, heuristic=self._heuristic)\n",
    "        return info\n",
    "\n",
    "    def step(self, obs):\n",
    "        if not self._plan:\n",
    "            print(\"Warning: step was called without a plan. Defaulting to random action.\")\n",
    "            return self._rng.choice(self._actions)\n",
    "        return self._plan.pop(0)\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        if isinstance(self._heuristic, Heuristic):\n",
    "            self._heuristic.seed(seed)\n",
    "        \n",
    "    def train(self, env):\n",
    "        if isinstance(self._heuristic, Heuristic):\n",
    "            self._heuristic.train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AStar(Planner):\n",
    "    \"\"\"Planning with A* search\n",
    "    \"\"\"\n",
    "    \n",
    "    Node = namedtuple(\"Node\", [\"state\", \"parent\", \"action\", \"g\"])\n",
    "\n",
    "    def __init__(self, successor_fn, check_goal_fn, timeout=100):\n",
    "        self._get_successor_state = successor_fn\n",
    "        self._check_goal = check_goal_fn\n",
    "        self._heuristic = None\n",
    "        self._timeout = timeout\n",
    "        self._actions = None\n",
    "        \n",
    "    def __call__(self, state, heuristic=None, verbose=True):\n",
    "        self._heuristic = heuristic or (lambda node : 0)\n",
    "        return self._get_plan(state, verbose=verbose)\n",
    "\n",
    "    def set_actions(self, actions):\n",
    "        self._actions = actions\n",
    "        if isinstance(self._heuristic, Heuristic):\n",
    "            self._heuristic.set_actions(actions)\n",
    "\n",
    "    def _get_plan(self, state, verbose=True):\n",
    "        start_time = time.time()\n",
    "        queue = []\n",
    "        state_to_best_g = defaultdict(lambda : float(\"inf\"))\n",
    "        tiebreak = count()\n",
    "\n",
    "        root_node = self.Node(state=state, parent=None, action=None, g=0)\n",
    "        hq.heappush(queue, (self._get_priority(root_node), next(tiebreak), root_node))\n",
    "        num_expansions = 0\n",
    "\n",
    "        while len(queue) > 0 and (time.time() - start_time < self._timeout):\n",
    "            _, _, node = hq.heappop(queue)\n",
    "            # If we already found a better path here, don't bother\n",
    "            if state_to_best_g[node.state] < node.g:\n",
    "                continue\n",
    "            # If the goal holds, return\n",
    "            if self._check_goal(node.state):\n",
    "                if verbose:\n",
    "                    print(\"\\nPlan found!\")\n",
    "                return self._finish_plan(node), {'node_expansions' : num_expansions}\n",
    "            num_expansions += 1\n",
    "            if verbose:\n",
    "                print(f\"Expanding node {num_expansions}\", end='\\r', flush=True)\n",
    "            # Generate successors\n",
    "            for action, child_state in self._get_successors(node.state):\n",
    "                # If we already found a better path to child, don't bother\n",
    "                if state_to_best_g[child_state] <= node.g+1:\n",
    "                    continue\n",
    "                # Add new node\n",
    "                child_node = self.Node(state=child_state, parent=node, action=action, g=node.g+1)\n",
    "                priority = self._get_priority(child_node)\n",
    "                hq.heappush(queue, (priority, next(tiebreak), child_node))\n",
    "                state_to_best_g[child_state] = child_node.g\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Warning: planning failed.\")\n",
    "        return [], {'node_expansions' : num_expansions}\n",
    "    \n",
    "    def _get_successors(self, state):\n",
    "        for action in self._actions:\n",
    "            next_state = self._get_successor_state(state, action)\n",
    "            yield action, next_state\n",
    "\n",
    "    def _finish_plan(self, node):\n",
    "        plan = []\n",
    "        while node.parent is not None:\n",
    "            plan.append(node.action)\n",
    "            node = node.parent\n",
    "        plan.reverse()\n",
    "        return plan\n",
    "\n",
    "    def _get_priority(self, node):\n",
    "        h = self._heuristic(node)\n",
    "        if isinstance(h, tuple):\n",
    "            return (tuple(node.g + hi for hi in h), h)\n",
    "        return (node.g + h, h)\n",
    "\n",
    "\n",
    "class BestFirstSearch(AStar):\n",
    "    \"\"\"Planning with best-first search\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_priority(self, node):\n",
    "        h = self._heuristic(node)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizers\n",
    "We start you off with some featurizers. If you are interested, you are welcome to implement others. But feel free to just treat these as black boxes too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFeaturizer(Featurizer):\n",
    "    \"\"\"A tabular featurizer assigns a unique ID to each input.\n",
    "    \"\"\"\n",
    "    def __init__(self, one_hot=False):\n",
    "        self._one_hot = one_hot\n",
    "        self._x_to_idx = {}\n",
    "        self._idx_to_x = {}\n",
    "        self._num_features = 0\n",
    "        self._unknown_idx = None\n",
    "        self._initialized = False\n",
    "\n",
    "    def initialize(self, all_data):\n",
    "        for i, x in enumerate(sorted(set(all_data))):\n",
    "            self._x_to_idx[x] = i\n",
    "            self._idx_to_x[i] = x\n",
    "        self._num_features = max(self._idx_to_x) + 1\n",
    "        self._unknown_idx = self._num_features\n",
    "        self._initialized = True\n",
    "        print(f\"Initialized {self._num_features} tabular features\")\n",
    "\n",
    "    def apply(self, x):\n",
    "        assert self._initialized, \"Must call `initialize(all_data)` before `apply(datum)`.\"\n",
    "        x_id = self._x_to_idx.get(x, self._unknown_idx)\n",
    "        if self._one_hot:\n",
    "            xhat = np.zeros(self._num_features + 1, dtype=np.float32)\n",
    "            xhat[x_id] = True\n",
    "            return xhat\n",
    "        return x_id\n",
    "\n",
    "    def invert(self, xhat):\n",
    "        if self._one_hot:\n",
    "            assert sum(xhat) == 1\n",
    "            idx = np.argwhere(xhat)\n",
    "        else:\n",
    "            idx = xhat\n",
    "        return self._idx_to_x[idx]\n",
    "    \n",
    "\n",
    "class PropositionalFeaturizer(Featurizer):\n",
    "    \"\"\"A propositional featurizer creates a boolean vector with one dimension per fact (Literal).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._x_to_idx = {}\n",
    "        self._idx_to_x = {}\n",
    "        self._num_features = 0\n",
    "        self._initialized = False\n",
    "\n",
    "    @classmethod\n",
    "    def _wrap_goal_literal(cls, x):\n",
    "        if isinstance(x, Predicate):\n",
    "            return Predicate(\"WANT\"+x.name, x.arity, var_types=x.var_types,\n",
    "                is_negative=x.is_negative, is_anti=x.is_anti)\n",
    "        new_predicate = cls._wrap_goal_literal(x.predicate)\n",
    "        return new_predicate(*x.variables)\n",
    "\n",
    "    def _preproc_pddl_state(self, X):\n",
    "        if isinstance(X, State):\n",
    "            return X.literals | {self._wrap_goal_literal(x) for x in X.goal.literals}\n",
    "        return X\n",
    "\n",
    "    def initialize(self, all_data):\n",
    "        all_props = { x for X in all_data for x in self._preproc_pddl_state(X) }\n",
    "        for i, x in enumerate(sorted(all_props)):\n",
    "            self._x_to_idx[x] = i\n",
    "            self._idx_to_x[i] = x\n",
    "        self._num_features = max(self._idx_to_x)+1\n",
    "        self._initialized = True\n",
    "        print(f\"Initialized {self._num_features} propositional features\")\n",
    "\n",
    "    def apply(self, X):\n",
    "        assert self._initialized, \"Must call `initialize(all_data)` before `apply(datum)`.\"\n",
    "        X = self._preproc_pddl_state(X)\n",
    "        vec = np.zeros(self._num_features, dtype=np.float32)\n",
    "        for x in X:\n",
    "            try:\n",
    "                idx = self._x_to_idx[x]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            vec[idx] = 1\n",
    "        return vec\n",
    "\n",
    "    def invert(self, vec):\n",
    "        return { self._idx_to_x[idx] for idx in np.argwhere(vec) }\n",
    "    \n",
    "\n",
    "class SARStateFeaturizer(Featurizer):\n",
    "    \"\"\"This featurizer is specific to Search And Rescue states.\n",
    "    It gives the dictionary-like state features that we saw in the previous homework.\n",
    "    \"\"\"\n",
    "    def initialize(self, all_data):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def apply(cls, internal_state):\n",
    "        state = { \"carrying\" : None }\n",
    "        state[\"rescue\"] = set()\n",
    "        for lit in internal_state.goal.literals:\n",
    "            state[\"rescue\"].add(lit.variables[0].name)\n",
    "        state[\"rescue\"] = frozenset(state[\"rescue\"]) # make hashable\n",
    "        for lit in internal_state.literals:\n",
    "            if lit.predicate.name.endswith(\"at\"):\n",
    "                obj_name = lit.variables[0].name\n",
    "                r, c = cls._loc_to_rc(lit.variables[1])\n",
    "                state[obj_name] = (r, c)\n",
    "            if lit.predicate.name == \"carrying\":\n",
    "                person_name = lit.variables[1].name\n",
    "                state[\"carrying\"] = person_name\n",
    "        state = tuple(sorted(state.items())) # make hashable\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _loc_to_rc(loc_str):\n",
    "        assert loc_str.startswith(\"f\") and loc_str.endswith(\"f\")\n",
    "        r, c = loc_str[1:-1].split('-')\n",
    "        return (int(r), int(c))\n",
    "    \n",
    "\n",
    "class SARMinimalStateFeaturizer(Featurizer):\n",
    "    \"\"\"This featurizer is specific to Search And Rescue states.\n",
    "    It puts the positions of the robot, hospital, and people into a normalized\n",
    "    vector and ignores the walls. It also includes bits for whether each person\n",
    "    is being carried and whether each person needs rescue.\n",
    "    \"\"\"\n",
    "    # for normalization\n",
    "    max_location = 6\n",
    "    \n",
    "    def initialize(self, all_data):\n",
    "        pass\n",
    "    \n",
    "    def apply(self, x):\n",
    "        sar_state = dict(SARStateFeaturizer.apply(x))\n",
    "        state = []\n",
    "        # add robot position\n",
    "        state.extend(sar_state[\"robot0\"])\n",
    "        # add hospital position\n",
    "        state.extend(sar_state[\"hospital0\"])\n",
    "        # get people\n",
    "        people = sorted({ k for k in sar_state if k.startswith(\"person\")})\n",
    "        if sar_state[\"carrying\"]:\n",
    "            people.append(sar_state[\"carrying\"])\n",
    "            people.sort()\n",
    "        # for each person...\n",
    "        for person in people:\n",
    "            # check whether the person is being carried\n",
    "            if sar_state.get(\"carrying\", None) == person:\n",
    "                # add whether the person is being carried\n",
    "                state.append(1.)\n",
    "                # add the persons location (= robot's location)\n",
    "                state.extend(sar_state[\"robot0\"])\n",
    "            else:\n",
    "                # add whether the person is being carried\n",
    "                state.append(0.)\n",
    "                # add the persons location\n",
    "                state.extend(sar_state[person])\n",
    "            # add whether the person needs rescue\n",
    "            state.append(float(person in sar_state[\"rescue\"]))\n",
    "        # normalize\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        state = (state / self.max_location) - 0.5\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches\n",
    "Implement your own approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLearningApproach1(Approach):\n",
    "    \"\"\"TODO: implement me!\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\"Implement me! You may want to add args or kwargs.\")\n",
    "\n",
    "    def set_actions(self, actions):\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def train(self, env):\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def reset(self, state):\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def step(self, obs):\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def seed(self, seed):\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register your approaches\n",
    "Give your approaches names in the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_approach(name, env, planning_timeout=10):\n",
    "    \"\"\"Put new approaches here!\n",
    "    \"\"\"\n",
    "    if name == \"random\":\n",
    "        return RandomActions()\n",
    "    \n",
    "    if name == \"astar_uniform\":\n",
    "        planner = AStar(env.get_successor_state, env.check_goal, timeout=planning_timeout)\n",
    "        return SearchApproach(planner=planner)\n",
    "    \n",
    "    if name == \"my_learning_approach1\":\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "        \n",
    "    raise Exception(f\"Unrecognized approach: {name}\")\n",
    "\n",
    "# Add your approach names here\n",
    "approaches = [\n",
    "    \"random\",\n",
    "    \"astar_uniform\",\n",
    "#     \"my_learning_approach1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Pipeline\n",
    "Here's all the code that you should need to evaluate your approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_test(test_env, problem_idx, model, max_horizon=250, max_duration=10):\n",
    "    print(f\"Running test problem {problem_idx} in environment {test_env.spec.id}\")\n",
    "    test_env.fix_problem_index(problem_idx)\n",
    "    start_time = time.time()\n",
    "    obs, info = test_env.reset()\n",
    "    model_info = model.reset(obs)\n",
    "    node_expansions = model_info.get('node_expansions', 0)\n",
    "    num_steps = 0\n",
    "    success = False\n",
    "    for t in range(max_horizon):\n",
    "        if time.time() - start_time > max_duration:\n",
    "            break\n",
    "        print(\".\", end='', flush=True)\n",
    "        act = model.step(obs)\n",
    "        obs, reward, done, info = test_env.step(act)\n",
    "        num_steps += 1\n",
    "        if done:\n",
    "            assert reward == 1\n",
    "            success = True\n",
    "            break\n",
    "    duration = time.time() - start_time\n",
    "    print(f\" final duration: {duration} with num steps {num_steps} and success={success}.\")\n",
    "    return duration, num_steps, node_expansions, success\n",
    "\n",
    "def run_single_experiment(model, train_env, test_env, seed=0):\n",
    "    # Initialize\n",
    "    test_env.reset()\n",
    "    actions = test_env.get_possible_actions()\n",
    "    model.set_actions(actions)\n",
    "    model.seed(seed)\n",
    "    \n",
    "    # Training\n",
    "    training_start_time = time.time()\n",
    "    model.train(train_env)\n",
    "    train_duration = time.time() - training_start_time\n",
    "    train_durations = [train_duration] * len(test_env.problems) # for result reporting convenience\n",
    "\n",
    "    # Test time\n",
    "    test_durations = [] # seconds, one per problem\n",
    "    test_num_steps = [] # integers\n",
    "    test_node_expansions = [] # integers\n",
    "    test_successes = [] # boolean, True if successful\n",
    "    \n",
    "    for problem_idx in range(len(test_env.problems)):\n",
    "        duration, num_steps, node_expansions, success = \\\n",
    "            run_single_test(test_env, problem_idx, model)\n",
    "        test_durations.append(duration)\n",
    "        test_num_steps.append(num_steps)\n",
    "        test_node_expansions.append(node_expansions)\n",
    "        test_successes.append(success)\n",
    "\n",
    "    return train_durations, test_durations, test_num_steps, test_node_expansions, test_successes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's where the action happens\n",
    "If you are impatient like me, you should feel free to change the levels list below to run only the first few levels, especially while you are developing your code. If you find that your approaches do not work at all after, e.g., level 3, then you do not need to run them on levels 4, 5, 6.\n",
    "\n",
    "\n",
    "**Footnote** Last week we used \"SearchAndRescueLevel1-v0\". This week we are using \"PDDLSearchAndRescueLevel-v0\" instead. The difference is in the state representation. \"SearchAndRescueLevel1-v0\" used a dictionary-like state representation that is easy to read and interpret. \"PDDLSearchAndRescueLevel1-v0\" uses a state representation based on \"Literals\", which is a concept we will revisit in later weeks and you need not worry about now. If you want to recover the state representation from last week, you can use the `SARStateFeaturizer` above. Note that your code from last week will probably be much faster on the \"PDDL\" version of the environment because successor generation is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = list(range(1, 7))\n",
    "\n",
    "all_results = {}\n",
    "for level in levels:\n",
    "    all_results[level] = {}\n",
    "    train_env = pddlgym.make(f\"PDDLSearchAndRescueLevel{level}-v0\")\n",
    "    test_env = pddlgym.make(f\"PDDLSearchAndRescueLevel{level}Test-v0\")\n",
    "    for approach in approaches:\n",
    "        all_results[level][approach] = []\n",
    "        model = get_approach(approach, test_env)\n",
    "        results = run_single_experiment(model, train_env, test_env)\n",
    "        for (train_dur, dur, num_steps, num_nodes, succ) in zip(*results):\n",
    "            all_results[level][approach].append((train_dur, dur, num_steps, num_nodes, succ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Approach\", \"Train Time\", \"Duration\", \"Num Steps\", \"Num Nodes\", \"Successes\"]\n",
    "\n",
    "for level in sorted(all_results):\n",
    "    print(f\"\\n### LEVEL {level} ###\")\n",
    "    mean_table = [(a, ) + tuple(np.mean(all_results[level][a], axis=0)) for a in sorted(all_results[level])]\n",
    "    std_table = [(a, ) + tuple(np.std(all_results[level][a], axis=0)) for a in sorted(all_results[level])]\n",
    "    print(\"\\n# Means #\")\n",
    "    print(tabulate(mean_table, headers=columns))\n",
    "    print(\"\\n# Standard Deviations #\")\n",
    "    print(tabulate(std_table, headers=columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW1.1_starter_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
