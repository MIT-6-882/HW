{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.882 HW 4.2 Starter Code\n",
    "\n",
    "See the problem set handout for instructions and deliverables.\n",
    "\n",
    "See HW1.1 Starter Code for dependency installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run this once ever 12 hours)\n",
    "!git clone https://github.com/MIT-6-882/HW utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.planning_utils import AStar\n",
    "import abc\n",
    "import functools\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalDist:\n",
    "    \"\"\"A hashable categorical distribution.\n",
    "\n",
    "    Examples:\n",
    "    >>> dist = CategoricalDist({\"a\" : 0.3, \"b\" : 0.7})\n",
    "    >>> print(dist[\"a\"])\n",
    "    >>> print(dist.items())\n",
    "    >>> print(dist.sample(np.random))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cat_to_prob : {hashable : float}\n",
    "        Maps a category to a probability. Needs to sum to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, cat_to_prob):\n",
    "        assert abs(sum(cat_to_prob.values()) - 1.) < 1e-6\n",
    "        self._dict = {k : p for k, p in cat_to_prob.items() if p > 0}\n",
    "        self._hashable = tuple(sorted(self._dict.items()))\n",
    "        self._hash = hash(self._hashable)\n",
    "        self._str = \"\\n\".join([f\"{k}:{v}\" for k,v in self._hashable])\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self._hashable == other._hashable\n",
    "\n",
    "    def __str__(self):\n",
    "        return self._str\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._str\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._dict)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        try:\n",
    "            return self._dict[key]\n",
    "        except KeyError:\n",
    "            return 0.\n",
    "\n",
    "    def items(self):\n",
    "        return self._dict.items()\n",
    "\n",
    "    def sample(self, rng):\n",
    "        choices, probs = zip(*self._dict.items())\n",
    "        choice_idx = rng.choice(len(choices), p=probs)\n",
    "        return choices[choice_idx]\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def entropy(self):\n",
    "        H = sum(-p * np.log(p) for p in self._dict.values())\n",
    "        return H\n",
    "    \n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def state_estimator(pomdp, belief_state, action, obs):\n",
    "    \"\"\"Get a new belief state from the previous belief state (time t),\n",
    "    last action taken (time t), and the observation received (time t+1).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pomdp : POMDP\n",
    "    belief_state : CategoricalDist\n",
    "    action : hashable\n",
    "    obs : hashable\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    next_belief_state : CategoricalDist\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POMDPs (Environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDP:\n",
    "    \"\"\"An environment API that exposes transition probabilities\n",
    "    and observation probabilities for use with planners.\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def get_all_states(self):\n",
    "        \"\"\"Return a list of all possible states of the environment.\n",
    "\n",
    "        We're restricted to small environments because we need\n",
    "        this function.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states : [ hashable ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_all_observations(self):\n",
    "        \"\"\"Return a list of all possible observations of the\n",
    "        environment.\n",
    "\n",
    "        We're restricted to small environments because we need\n",
    "        this function.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        observations : [ hashable ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_all_actions(self):\n",
    "        \"\"\"Return a list of all possible actions of the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        actions : [ hashable ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        \"\"\"Return a dictionary of next_states to probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        next_states : CategoricalDist\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_observation_probabilities(self, state, action=None):\n",
    "        \"\"\"Return a dictionary of observations to probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        observations : CategoricalDist\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "            Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_initial_state_distribution(self):\n",
    "        \"\"\"Distribution over initial states.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dist : CategoricalDist\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_maximum_possible_reward(self):\n",
    "        \"\"\"Not part of the POMDP formalism, but included so\n",
    "        we can use conversions to shortest path.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "            Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "\n",
    "\n",
    "class TigerPOMDP(POMDP):\n",
    "    \"\"\"The tiger problem discussed in class\n",
    "    \"\"\"\n",
    "    def get_all_states(self):\n",
    "        return [\"tiger-left\", \"tiger-right\"]\n",
    "\n",
    "    def get_all_observations(self):\n",
    "        # The null observation is just for the initial state\n",
    "        return [\"null\", \"hear-left\", \"hear-right\"]\n",
    "\n",
    "    def get_all_actions(self):\n",
    "        return [\"listen\", \"open-left\", \"open-right\"]\n",
    "\n",
    "    def get_initial_state_distribution(self):\n",
    "        return CategoricalDist({\"tiger-left\" : 0.51, \"tiger-right\" : 0.49})\n",
    "\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        if action == \"listen\":\n",
    "            return CategoricalDist({state : 1.0})\n",
    "        # Return to beginning with random resets\n",
    "        assert action in [\"open-left\", \"open-right\"]\n",
    "        return self.get_initial_state_distribution()\n",
    "\n",
    "    def get_observation_probabilities(self, state, action=None):\n",
    "        # Initial or final state\n",
    "        if action in [None, \"open-left\", \"open-right\"]:\n",
    "            return CategoricalDist({\"null\" : 1.0})\n",
    "        assert action == \"listen\", f\"Unknown action {action}\"\n",
    "        if state == \"tiger-right\":\n",
    "            return CategoricalDist({\"hear-right\" : 0.85, \"hear-left\" : 0.15})\n",
    "        if state == \"tiger-left\":\n",
    "            return CategoricalDist({\"hear-left\" : 0.85, \"hear-right\" : 0.15})\n",
    "        raise Exception(f\"Unknown state {state}\")\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        if action == \"listen\":\n",
    "            return -1\n",
    "        if action == \"open-left\":\n",
    "            if state == \"tiger-left\":\n",
    "                return -100\n",
    "            assert state == \"tiger-right\"\n",
    "            return 10\n",
    "        if action == \"open-right\":\n",
    "            if state == \"tiger-right\":\n",
    "                return -100\n",
    "            assert state == \"tiger-left\"\n",
    "            return 10\n",
    "        raise Exception(f\"Unknown action {action}\")\n",
    "\n",
    "    def get_maximum_possible_reward(self):\n",
    "        return 10\n",
    "\n",
    "\n",
    "class LemonPOMDP(POMDP):\n",
    "    \"\"\"The 'is the car a lemon?' POMDP from class.\n",
    "\n",
    "    Very similar to tiger!\n",
    "    \"\"\"\n",
    "    def get_all_states(self):\n",
    "        return [\"lemon\", \"peach\"]\n",
    "\n",
    "    def get_all_observations(self):\n",
    "        return [\"null\", \"pass\", \"fail\"]\n",
    "\n",
    "    def get_all_actions(self):\n",
    "        return [\"buy\", \"dont-buy\", \"inspect\"]\n",
    "\n",
    "    def get_initial_state_distribution(self):\n",
    "        return CategoricalDist({\"lemon\" : 0.2, \"peach\" : 0.8})\n",
    "\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        if action == \"inspect\":\n",
    "            return CategoricalDist({state : 1.0})\n",
    "        # Return to beginning with random resets\n",
    "        assert action in [\"buy\", \"dont-buy\"]\n",
    "        return self.get_initial_state_distribution()\n",
    "\n",
    "    def get_observation_probabilities(self, state, action=None):\n",
    "        # Initial or final state\n",
    "        if action in [None, \"buy\", \"dont-buy\"]:\n",
    "            return CategoricalDist({\"null\" : 1.0})\n",
    "        assert action == \"inspect\", f\"Unknown action {action}\"\n",
    "        if state == \"lemon\":\n",
    "            return CategoricalDist({\"pass\" : 0.4, \"fail\" : 0.6})\n",
    "        if state == \"peach\":\n",
    "            return CategoricalDist({\"pass\" : 0.9, \"hear-right\" : 0.1})\n",
    "        raise Exception(f\"Unknown state {state}\")\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        if action == \"inspect\":\n",
    "            return -9\n",
    "        if action == \"buy\":\n",
    "            if state == \"lemon\":\n",
    "                return -100\n",
    "            assert state == \"peach\"\n",
    "            return 60\n",
    "        if action == \"dont-buy\":\n",
    "            return 0\n",
    "        raise Exception(f\"Unknown action {action}\")\n",
    "\n",
    "    def get_maximum_possible_reward(self):\n",
    "        return 60\n",
    "\n",
    "\n",
    "class SARPOMDP(POMDP):\n",
    "    \"\"\"Search and rescue pomdp\n",
    "    \"\"\"\n",
    "    # Set up actions\n",
    "    ALL_ACTIONS = MOVE_UP, MOVE_DOWN, MOVE_LEFT, MOVE_RIGHT, \\\n",
    "                  SCAN_UP, SCAN_DOWN, SCAN_LEFT, SCAN_RIGHT = range(8)\n",
    "\n",
    "    # Set up the grid cell types\n",
    "    EMPTY, WALL, FIRE, HIDDEN = range(4)\n",
    "\n",
    "    # Set up possible scan responses (NA = not applicable)\n",
    "    SCAN_RESPONSES = GOT_RESPONSE, GOT_NO_RESPONSE, NA = range(3)\n",
    "\n",
    "    # Set up stochasticity parameters\n",
    "    MOVE_NOISE_PROB = 0.1\n",
    "    SCAN_NOISE_PROB = 0.1\n",
    "\n",
    "    # Set up rewards\n",
    "    TIME_REWARD = -1\n",
    "    RESCUE_REWARD = 100\n",
    "    FIRE_REWARD = -100\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.GRID.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.GRID.shape[1]\n",
    "    \n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_all_states(self):\n",
    "        \"\"\"States are (robot_loc, person_loc)\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        for person_loc in np.argwhere(self.GRID == self.HIDDEN):\n",
    "            person_loc = tuple(person_loc)\n",
    "            for possible_robot_loc in self._get_possible_robot_locs():\n",
    "                states.append((possible_robot_loc, person_loc))\n",
    "        return states\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_all_observations(self):\n",
    "        \"\"\"Observations are (robot_loc, scan_response)\n",
    "        \"\"\"\n",
    "        observations = []\n",
    "        for scan_response in self.SCAN_RESPONSES:\n",
    "            for possible_robot_loc in self._get_possible_robot_locs():\n",
    "                observations.append((possible_robot_loc, scan_response))\n",
    "        return observations\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_all_actions(self):\n",
    "        \"\"\"Actions are moving and scanning in four directions\n",
    "        \"\"\"\n",
    "        return self.ACTIONS\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_initial_state_distribution(self):\n",
    "        \"\"\"We will always start the robot in the same initial place,\n",
    "        and the distribution over rooms will be uniform\n",
    "        \"\"\"\n",
    "        initial_robot_loc = self.INITIAL_ROBOT_LOC\n",
    "        initial_states = []\n",
    "        for person_loc in np.argwhere(self.GRID == self.HIDDEN):\n",
    "            person_loc = tuple(person_loc)\n",
    "            initial_states.append((initial_robot_loc, person_loc))\n",
    "        dist = {s : 1./len(initial_states) for s in initial_states}\n",
    "        return CategoricalDist(dist)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        \"\"\"Movement effects are stochastic\n",
    "        \"\"\"\n",
    "        # Scanning does not change the state\n",
    "        if action in [self.SCAN_UP, self.SCAN_DOWN, self.SCAN_LEFT,\n",
    "                      self.SCAN_RIGHT]:\n",
    "            return CategoricalDist({state : 1.0})\n",
    "        robot_r, robot_c = state[0]\n",
    "        # Moving is deterministically null if we're in a fire\n",
    "        if self.GRID[robot_r, robot_c] == self.FIRE:\n",
    "            return CategoricalDist({state : 1.0})\n",
    "        # Moving is stochastic if we're not in a fire\n",
    "        if action == self.MOVE_UP:\n",
    "            intended_delta = (-1, 0)\n",
    "        elif action == self.MOVE_DOWN:\n",
    "            intended_delta = (1, 0)\n",
    "        elif action == self.MOVE_LEFT:\n",
    "            intended_delta = (0, -1)\n",
    "        elif action == self.MOVE_RIGHT:\n",
    "            intended_delta = (0, 1)\n",
    "        else:\n",
    "            raise Exception(f\"Unrecognized action {action}.\")\n",
    "        # Reset to the beginning if we're moving into a goal\n",
    "        next_loc = (robot_r + intended_delta[0],\n",
    "                    robot_c + intended_delta[1])\n",
    "        if next_loc == state[1]:\n",
    "            return self.get_initial_state_distribution()\n",
    "        dist = defaultdict(float)\n",
    "        for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            if self._is_valid_robot_loc((robot_r+dr, robot_c+dc)):\n",
    "                next_robot_loc = (robot_r + dr, robot_c + dc)\n",
    "            else:\n",
    "                next_robot_loc = (robot_r, robot_c)\n",
    "            next_state = (next_robot_loc, state[1])\n",
    "            if (dr, dc) == intended_delta:\n",
    "                dist[next_state] += 1.-self.MOVE_NOISE_PROB\n",
    "            else:\n",
    "                dist[next_state] += self.MOVE_NOISE_PROB / 3.\n",
    "        return CategoricalDist(dist)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_observation_probabilities(self, state, action=None):\n",
    "        \"\"\"Always observe robot loc directly; get scan response\n",
    "        stochastically when scanning\n",
    "        \"\"\"\n",
    "        # If the action was not a scan, deterministic\n",
    "        if action not in [self.SCAN_UP, self.SCAN_DOWN,\n",
    "                          self.SCAN_LEFT, self.SCAN_RIGHT]:\n",
    "            return CategoricalDist({(state[0], self.NA) : 1.0})\n",
    "        # If the action was a scan, get a noisy response\n",
    "        # First figure out the 'correct' directions to scan\n",
    "        correct_scans = []\n",
    "        # Delta between person and robot\n",
    "        dr, dc = np.subtract(state[1], state[0])\n",
    "        if dr < 0:\n",
    "            correct_scans.append(self.SCAN_UP)\n",
    "        elif dr > 0:\n",
    "            correct_scans.append(self.SCAN_DOWN)\n",
    "        if dc < 0:\n",
    "            correct_scans.append(self.SCAN_LEFT)\n",
    "        elif dc > 0:\n",
    "            correct_scans.append(self.SCAN_RIGHT)\n",
    "        assert len(correct_scans) <= 2\n",
    "        if action in correct_scans:\n",
    "            correct_response = self.GOT_RESPONSE\n",
    "            incorrect_response = self.GOT_NO_RESPONSE\n",
    "        else:\n",
    "            correct_response = self.GOT_NO_RESPONSE\n",
    "            incorrect_response = self.GOT_RESPONSE\n",
    "        # Return a noisy response\n",
    "        dist = {\n",
    "            (state[0], correct_response) : 1.-self.SCAN_NOISE_PROB,\n",
    "            (state[0], incorrect_response) : self.SCAN_NOISE_PROB,\n",
    "        }\n",
    "        return CategoricalDist(dist)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_reward(self, state, action):\n",
    "        \"\"\"Get a positive reward for rescuing the person\n",
    "        \"\"\"\n",
    "        robot_loc, person_loc = state\n",
    "        if action == self.MOVE_UP:\n",
    "            intended_delta = (-1, 0)\n",
    "        elif action == self.MOVE_DOWN:\n",
    "            intended_delta = (1, 0)\n",
    "        elif action == self.MOVE_LEFT:\n",
    "            intended_delta = (0, -1)\n",
    "        elif action == self.MOVE_RIGHT:\n",
    "            intended_delta = (0, 1)\n",
    "        else:\n",
    "            return self.TIME_REWARD\n",
    "        next_loc = (robot_loc[0] + intended_delta[0],\n",
    "                    robot_loc[1] + intended_delta[1])\n",
    "        if next_loc == person_loc:\n",
    "            return self.RESCUE_REWARD\n",
    "        # If in fire, big negative reward\n",
    "        if 0 <= next_loc[0] < self.height and \\\n",
    "           0 <= next_loc[1] < self.width and \\\n",
    "           self.GRID[next_loc[0], next_loc[1]]:\n",
    "            return self.FIRE_REWARD\n",
    "        return self.TIME_REWARD\n",
    "\n",
    "    def get_maximum_possible_reward(self):\n",
    "        return self.RESCUE_REWARD\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def _get_possible_robot_locs(self):\n",
    "        \"\"\"Helper for state and obs spaces\n",
    "        \"\"\"\n",
    "        mask = (self.GRID == self.EMPTY) \n",
    "        mask |= (self.GRID == self.HIDDEN)\n",
    "        mask |= (self.GRID == self.FIRE)\n",
    "        return [tuple(loc) for loc in np.argwhere(mask)]\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def _is_valid_robot_loc(self, robot_loc):\n",
    "        \"\"\"Helper for transitions\n",
    "        \"\"\"\n",
    "        robot_r, robot_c = robot_loc\n",
    "        if not (0 <= robot_r < self.height and \\\n",
    "                0 <= robot_c < self.width):\n",
    "            return False\n",
    "        if self.GRID[robot_r,robot_c] == self.WALL:\n",
    "            return False\n",
    "        assert robot_loc in self._get_possible_robot_locs()\n",
    "        return True\n",
    "\n",
    "\n",
    "class TinySARPOMDP(SARPOMDP):\n",
    "    E, W, F, H = SARPOMDP.EMPTY, SARPOMDP.WALL, SARPOMDP.FIRE, \\\n",
    "                 SARPOMDP.HIDDEN\n",
    "    GRID = np.array([\n",
    "        [H, E, E, E, E, E, H]\n",
    "    ])\n",
    "    INITIAL_ROBOT_LOC = (0, 3)\n",
    "    # Override actions to make this easier\n",
    "    ACTIONS = [SARPOMDP.MOVE_LEFT, SARPOMDP.MOVE_RIGHT,\n",
    "               SARPOMDP.SCAN_LEFT] \n",
    "    # Override noise prob to make this easier\n",
    "    MOVE_NOISE_PROB = 0.0\n",
    "\n",
    "\n",
    "class LargeSARPOMDP(SARPOMDP):\n",
    "    E, W, F, H = SARPOMDP.EMPTY, SARPOMDP.WALL, SARPOMDP.FIRE, \\\n",
    "                 SARPOMDP.HIDDEN\n",
    "    GRID = np.array([\n",
    "        [W, W, H, E, E, E, E],\n",
    "        [W, F, E, W, E, W, E],\n",
    "        [F, F, E, E, E, E, H],\n",
    "        [E, F, E, E, E, F, E],\n",
    "        [H, E, E, E, E, F, F],\n",
    "        [E, W, E, W, E, F, W],\n",
    "        [E, E, E, E, H, W, W]\n",
    "    ])\n",
    "    INITIAL_ROBOT_LOC = (3, 3)\n",
    "    ACTIONS = SARPOMDP.ALL_ACTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDPApproach:\n",
    "    \"\"\"An agent that maintains a belief state (distribution over\n",
    "    possible states) as it takes actions and receives observations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pomdp : POMDP\n",
    "        A pomdp.\n",
    "    \"\"\"\n",
    "    def __init__(self, pomdp, **kwargs):\n",
    "        self._pomdp = pomdp\n",
    "        self._rng = None # set in seed\n",
    "        self._belief_state = None # set in reset\n",
    "        self._last_action = None # set in step\n",
    "        self._states = pomdp.get_all_states()\n",
    "        self._actions = pomdp.get_all_actions()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Tell the agent that we have started a new problem.\n",
    "        \"\"\"\n",
    "        self._belief_state = None\n",
    "        self._last_action = None\n",
    "\n",
    "    def step(self, obs):\n",
    "        \"\"\"Receive an observation and produce an action to be\n",
    "        immediately executed in the environment.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : hashable\n",
    "            The observation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The action is assumed to be immediately taken.\n",
    "        \"\"\"\n",
    "        # If this is the first observation, we just need to\n",
    "        # reset the belief by inverting the observation model\n",
    "        if self._last_action is None:\n",
    "            self._belief_state = self._update_initial_belief(obs)\n",
    "        # Otherwise, we should use state_estimator to update\n",
    "        # the belief state given the previous action and obs\n",
    "        else:\n",
    "            self._belief_state = state_estimator(self._pomdp,\n",
    "                self._belief_state, self._last_action, obs)\n",
    "        # Find an action\n",
    "        action = self.belief_state_to_action(self._belief_state)\n",
    "        self._last_action = action\n",
    "        return action\n",
    "\n",
    "    def seed(self, seed):\n",
    "        \"\"\"Seed the agent, just in case it's random\n",
    "        \"\"\"\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "\n",
    "    def _update_initial_belief(self, obs):\n",
    "        \"\"\"Get a new initial belief given the obs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : hashable\n",
    "            The observation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        belief : CategoricalDist\n",
    "        \"\"\"\n",
    "        # Pr(s | o) = Pr(o | s)Pr(s) / Pr(o)\n",
    "        # Pr(o) = \\sum_s' Pr(o | s')Pr(s')\n",
    "        initial_state_dist = self._pomdp.get_initial_state_distribution()\n",
    "        F = self._pomdp.get_observation_probabilities\n",
    "        p_obs = sum([F(s)[obs] * p_s \\\n",
    "                     for s, p_s in initial_state_dist.items()])\n",
    "        belief = {s : F(s)[obs] * p_s / p_obs \\\n",
    "                  for s, p_s in initial_state_dist.items()}\n",
    "        return CategoricalDist(belief)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def belief_state_to_action(self, belief_state):\n",
    "        \"\"\"Return an action to be immediately taken, based on the current\n",
    "        belief state (self._belief_state). This is the main thing that\n",
    "        differentiates subclasses.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        belief_state : CategoricalDist\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The action to be taken immediately.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomActions(POMDPApproach):\n",
    "    \"\"\"Take random actions\n",
    "    \"\"\"\n",
    "    def belief_state_to_action(self, _):\n",
    "        return self._rng.choice(self._actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Likely Observation + A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLOAStar(POMDPApproach):\n",
    "    \"\"\"Plan using the most likely observation determinization\n",
    "    strategy. Do receding horizon control and replan at every\n",
    "    time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, pomdp, receding_horizon=25):\n",
    "        super().__init__(pomdp)\n",
    "        self._receding_horizon = receding_horizon\n",
    "        self._planner = self._create_planner()\n",
    "\n",
    "    def _create_planner(self):\n",
    "        \"\"\"Initializes the planner that uses state estimator as\n",
    "        its successor function to plan in belief space.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        planner : AStar\n",
    "        \"\"\"\n",
    "        # Start by creating the belief state successor function\n",
    "        mlo_succ = self._create_mlo_successor_fn(self._pomdp)\n",
    "        # Since we're doing finite horizon A* shortest path,\n",
    "        # we need to know the current depth in the state\n",
    "        successor_fn = lambda bh,a : (mlo_succ(bh[0], a), bh[1] + 1)\n",
    "        # A goal is when we've reached the horizon\n",
    "        check_goal_fn = lambda bh : (bh[1] >= self._receding_horizon)\n",
    "        # Cost is Rmax - expected reward\n",
    "        bel_rew = self._create_belief_reward_fn(self._pomdp)\n",
    "        r_max = self._pomdp.get_maximum_possible_reward()\n",
    "        get_cost = lambda bh,a : r_max - bel_rew(bh[0], a)\n",
    "        # Get all actions\n",
    "        actions = self._pomdp.get_all_actions()\n",
    "        # Make the planner\n",
    "        return AStar(successor_fn, check_goal_fn,\n",
    "                     get_cost=get_cost, actions=actions)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_mlo_successor_fn(pomdp):\n",
    "        \"\"\"Set up the successor function for the planner\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pomdp : POMDP\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mlo_succ : fn\n",
    "            Map from belief state and action to belief state.\n",
    "        \"\"\"\n",
    "        def mlo_succ(b, a):\n",
    "            \"\"\"Most likely observation successor function.\n",
    "\n",
    "            Given the belief state b at time t, and the action a\n",
    "            taken at time t, return the belief state at time t+1.\n",
    "\n",
    "            From lecture notes:\n",
    "                successor(b, a) = SE(b, a, argmax_o P(o | b, a))\n",
    "\n",
    "            Important note: The observation o that we are maxing\n",
    "            over in that equation is the observation at time t+1,\n",
    "            whereas b and a are from time t! Computing the prob\n",
    "            P(o | b, a) will therefore require using the transition\n",
    "            model (in addition to the observation model).\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            b : CategoricalDist\n",
    "                The belief at time t\n",
    "            a : hashable\n",
    "                The action taken at time t\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            next_belief : CategoricalDist\n",
    "            \"\"\"\n",
    "            raise NotImplementedError(\"Implement me!\")\n",
    "        return mlo_succ\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_belief_reward_fn(pomdp):\n",
    "        \"\"\"Set up the reward function for the planner\n",
    "        \"\"\"\n",
    "        def belief_reward(b, a):\n",
    "            \"\"\"The reward for a belief state is the expectation\n",
    "            over rewards for the states.\n",
    "            \"\"\"\n",
    "            r = 0.\n",
    "            for s in b:\n",
    "                r += b[s] * pomdp.get_reward(s, a)\n",
    "            return r\n",
    "        return belief_reward\n",
    "\n",
    "    def belief_state_to_action(self, belief_state):\n",
    "        # Replan on every time step!\n",
    "        # Remember that with the conversion to A* shortest path,\n",
    "        # we need to include the time step in the state.\n",
    "        plan, _ = self._planner((belief_state, 0))\n",
    "        return plan[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_test(pomdp, approach, max_horizon=50, verbose=False, rng=np.random):\n",
    "    # Extract things from POMDP\n",
    "    initial_state_dist = pomdp.get_initial_state_distribution()\n",
    "    T = pomdp.get_transition_probabilities\n",
    "    F = pomdp.get_observation_probabilities\n",
    "    R = pomdp.get_reward\n",
    "    # Start the evaluation\n",
    "    trial_returns = 0.\n",
    "    approach.reset()\n",
    "    # Sample an initial state\n",
    "    state = initial_state_dist.sample(rng)\n",
    "    # Initialize last action to None\n",
    "    action = None\n",
    "    for _ in range(max_horizon):\n",
    "        # Sample an observation based on current state and\n",
    "        # last action taken\n",
    "        obs = F(state, action).sample(rng)\n",
    "        if verbose: print(\"obs:\", obs)\n",
    "        # Get action from the approach\n",
    "        action = approach.step(obs)\n",
    "        if verbose: print(\"action:\", action)\n",
    "        # Take action\n",
    "        next_state = T(state, action).sample(rng)\n",
    "        # Get reward\n",
    "        trial_returns += R(state, action)\n",
    "        # Update state\n",
    "        state = next_state\n",
    "    return trial_returns\n",
    "\n",
    "def run_single_experiment(pomdp, approach, verbose=False, max_horizon=100,\n",
    "                          num_trials=1, seed=0):\n",
    "    # Initialize\n",
    "    approach.seed(seed)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # Do testing\n",
    "    test_durations = [] # seconds, one per problem\n",
    "    test_returns = [] # float, cumulative rewards\n",
    "    for trial in range(num_trials):\n",
    "        if verbose: print(f\"STARTING TRIAL {trial}\")\n",
    "        start_time = time.time()\n",
    "        returns = run_single_test(pomdp, approach, max_horizon=max_horizon,\n",
    "                                  rng=rng, verbose=verbose)\n",
    "        duration = time.time() - start_time\n",
    "        if verbose: print(f\"TRIAL {trial} RETURNS: {returns}\")\n",
    "        test_durations.append(duration)\n",
    "        test_returns.append(returns)\n",
    "    return test_durations, test_returns\n",
    "\n",
    "def print_results_table(env_name, results_for_env):\n",
    "    print(f\"\\n### {env_name} ###\")\n",
    "    mean_table = [(a, ) + tuple(np.mean(results_for_env[a], axis=0)) \\\n",
    "                  for a in sorted(results_for_env)]\n",
    "    columns = [\"Approach\", \"Duration\", \"Returns\"]\n",
    "    print(tabulate(mean_table, headers=columns))\n",
    "\n",
    "\n",
    "def main():\n",
    "    verbose = False\n",
    "\n",
    "    approaches = {\n",
    "        \"Random\" : RandomActions,\n",
    "        \"MLO-A*\" : MLOAStar,\n",
    "    }\n",
    "\n",
    "    pomdps = {\n",
    "        \"Tiger\" : TigerPOMDP(),\n",
    "        \"Lemon\" : LemonPOMDP(),\n",
    "        \"Tiny SAR\" : TinySARPOMDP(),\n",
    "        # \"Large SAR\" : LargeSARPOMDP(),\n",
    "    }\n",
    "\n",
    "    num_trials_per_env = {\n",
    "        \"Tiger\" : 25,\n",
    "        \"Lemon\" : 25,\n",
    "        \"Tiny SAR\" : 10,\n",
    "        \"Large SAR\" : 10,\n",
    "    }\n",
    "\n",
    "    receding_horizon_per_env = {\n",
    "        \"Tiger\" : 5,\n",
    "        \"Lemon\" : 5,\n",
    "        \"Tiny SAR\" : 10,\n",
    "        \"Large SAR\" : 25,\n",
    "    }\n",
    "\n",
    "    max_horizon_per_env = {\n",
    "        \"Tiger\" : 10,\n",
    "        \"Lemon\" : 10,\n",
    "        \"Tiny SAR\" : 10,\n",
    "        \"Large SAR\" : 50,\n",
    "    }\n",
    "\n",
    "    all_results = {}\n",
    "    for env_name, pomdp in pomdps.items():\n",
    "        results_for_env = {}\n",
    "        all_results[env_name] = results_for_env\n",
    "        num_trials = num_trials_per_env[env_name]\n",
    "        receding_horizon = receding_horizon_per_env[env_name]\n",
    "        max_horizon = max_horizon_per_env[env_name]\n",
    "        for approach_name, approach_cls in approaches.items():\n",
    "            approach = approach_cls(pomdp, receding_horizon=receding_horizon)\n",
    "            results = run_single_experiment(pomdp, approach,\n",
    "                                            max_horizon=max_horizon,\n",
    "                                            num_trials=num_trials,\n",
    "                                            verbose=verbose)\n",
    "            results_for_env[approach_name] = list(zip(*results))\n",
    "\n",
    "        # Print per-environment results (because of impatience)\n",
    "        print_results_table(env_name, results_for_env)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\n\" + \"*\" * 80)\n",
    "    for env_name in pomdps:\n",
    "        print_results_table(env_name, all_results[env_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
