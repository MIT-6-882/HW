{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3.1 Solutions\n",
    "\n",
    "See pset 1 for dependency installation instructions and see the problem set for deliverables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run this once ever 12 hours)\n",
    "!pip install --upgrade git+https://github.com/tomsilver/pddlgym # Install most recent PDDLGym (must be from source!)\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pddlgym.rendering import sar_render_from_string_grid\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import binom\n",
    "import abc\n",
    "import numpy as np\n",
    "import itertools\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (MDPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"An environment API that exposes transition probabilities,\n",
    "    for use with stochastic planners like value iteration\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def get_all_states(self):\n",
    "        \"\"\"Return a list of all possible states of the environment.\n",
    "\n",
    "        We're restricted to small environments because we need\n",
    "        this function.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states : [ hashable ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_all_actions(self):\n",
    "        \"\"\"Return a list of all possible actions of the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        actions : [ hashable ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        \"\"\"Return a dictionary of next_states to probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        next_states : { hashable : float }\n",
    "            Maps next state to prob. Sums to 1.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "            Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_initial_states(self):\n",
    "        \"\"\"Designate certain states as initial states. Not\n",
    "        always part of the standard MDP formalism.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states : [ hashable ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states. Not\n",
    "        always part of the standard MDP formalism.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        is_terminal : bool\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug 1D Grid MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Debug1DGridMDP(MDP):\n",
    "    \"\"\"A 1D grid MDP for debugging. The grid is 1x5\n",
    "    and the agent is meant to start off in the middle.\n",
    "    There is +10 reward on the rightmost square, -10 on\n",
    "    the left. Actions are left and right. An action effect\n",
    "    is reversed with 10% probability.\n",
    "    \"\"\"\n",
    "    def get_all_states(self):\n",
    "        return [0, 1, 2, 3, 4]\n",
    "\n",
    "    def get_all_actions(self):\n",
    "        return [-1, 1] # left, right\n",
    "\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        assert state in self.get_all_states()\n",
    "        assert action in self.get_all_actions()\n",
    "        intended_effect = min(max(state + action, 0), 4)\n",
    "        opposite_effect = min(max(state - action, 0), 4)\n",
    "        assert (intended_effect != opposite_effect)\n",
    "        return {intended_effect : 0.9, opposite_effect : 0.1}\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state == 0:\n",
    "            return -10\n",
    "        if next_state == 4:\n",
    "            return 10\n",
    "        return -1 # living penalty\n",
    "\n",
    "    def get_initial_states(self):\n",
    "        return [2]\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        return state in [0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and Rescue MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utility functions\n",
    "def get_all_reachable_states(initial_states, actions, T):\n",
    "    \"\"\"Utility for deriving all states that are reachable\n",
    "    from the initial states with >0 probability\n",
    "    \"\"\"\n",
    "    reachable_states = set(initial_states)\n",
    "    queue = [s for s in initial_states]\n",
    "    while len(queue) > 0:\n",
    "        state = queue.pop()\n",
    "        for action in actions:\n",
    "            for next_state, prob in T(state, action).items():\n",
    "                if prob > 0 and next_state not in reachable_states | set(queue):\n",
    "                    queue.append(next_state)\n",
    "                    reachable_states.add(next_state)\n",
    "    return reachable_states\n",
    "\n",
    "def combine_factored_distributions(factored_dists):\n",
    "    \"\"\"Utility for creating a distribution of states from factored dists\n",
    "    \"\"\"\n",
    "    keys = list(factored_dists.keys())\n",
    "    choices = [list(factored_dists[k].items()) for k in keys]\n",
    "    distribution = {}\n",
    "    for choice in itertools.product(*choices):\n",
    "        state = frozenset([(key, arg[0]) for key, arg in zip(keys, choice)])\n",
    "        prob = np.exp(sum(np.log([arg[1] for arg in choice])))\n",
    "        distribution[state] = prob\n",
    "    assert abs(1. - sum(distribution.values())) < 1e-6\n",
    "    return distribution\n",
    "\n",
    "\n",
    "class SARMDP(MDP):\n",
    "    \"\"\"Parent class for search and rescue MDPs.\n",
    "    \"\"\"\n",
    "    UP, DOWN, LEFT, RIGHT = range(4)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _loc_on_fire(self, loc, state):\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def get_all_states(self):\n",
    "        # This is a fairly generic way to derive all states\n",
    "        return get_all_reachable_states(\n",
    "            self.get_initial_states(),\n",
    "            self.get_all_actions(),\n",
    "            self.get_transition_probabilities,\n",
    "        )\n",
    "\n",
    "    def get_all_actions(self):\n",
    "        return [self.UP, self.DOWN, self.LEFT, self.RIGHT]\n",
    "\n",
    "    def _allowed_robot_loc(self, r, c):\n",
    "        \"\"\"Helper for transitions\n",
    "        \"\"\"\n",
    "        if (r, c) in self.WALL_LOCS:\n",
    "            return False\n",
    "        return 0 <= r < self.HEIGHT and 0 <= c < self.WIDTH\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        next_state = dict(next_state)\n",
    "        # Reached person\n",
    "        if next_state['robot_loc'] == next_state['person_loc']:\n",
    "            return self.RESCUE_REWARD\n",
    "        # In fire\n",
    "        if self._loc_on_fire(next_state['robot_loc'], next_state):\n",
    "            return self.FIRE_REWARD\n",
    "        return self.LIVING_REWARD\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        state = dict(state) # make easy to query\n",
    "        # State is terminal if the person is at the robot\n",
    "        if state['robot_loc'] == state['person_loc']:\n",
    "            return True\n",
    "        # State is terminal if robot is in fire\n",
    "        if self._loc_on_fire(state['robot_loc'], state):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def render(self, state, mode='string'):\n",
    "        state = dict(state)\n",
    "        grid = np.full((self.HEIGHT, self.WIDTH), 'open', dtype=object)\n",
    "        for r, c in self.WALL_LOCS:\n",
    "            grid[r, c] = 'wall'\n",
    "        for r in range(self.HEIGHT):\n",
    "            for c in range(self.WIDTH):\n",
    "                if self._loc_on_fire((r, c), state):\n",
    "                    grid[r, c] = 'fire'\n",
    "        r, c = state['robot_loc']\n",
    "        grid[r, c] = 'robot'\n",
    "        r, c = state['person_loc']\n",
    "        grid[r, c] = 'person'\n",
    "        if mode == 'string':\n",
    "            for name in ['open', 'wall', 'fire', 'robot', 'person']:\n",
    "                grid[grid == name] = name[0].upper()\n",
    "            return '\\n'.join([''.join(row) for row in grid])\n",
    "        assert mode == 'rgb'\n",
    "        return sar_render_from_string_grid(grid)\n",
    "\n",
    "\n",
    "class SARStochasticFiresMDP(SARMDP):\n",
    "    \"\"\"An MDP for search and rescue, with fires that appear\n",
    "    and disappear stochastically, and a person who moves around\n",
    "    stochastically in a middle \"room\". The goal is to rescue\n",
    "    the person.\n",
    "\n",
    "    The layout is:\n",
    "\n",
    "        OOOROOO\n",
    "        OWWWWWO\n",
    "        OFOPOFO\n",
    "        OWWWWWO\n",
    "\n",
    "    where O is empty space, W is wall, F is fire (that may appear\n",
    "    or disappear at that spot stochastically), and P is person (that\n",
    "    may move stochastically between the three spaces in the middle)\n",
    "    and R is the robot.\n",
    "\n",
    "    The probability that a fire switches from on to off is 0.05.\n",
    "    The probability that the person moves to an adjacent cell among\n",
    "    the 3 is 0.25.\n",
    "\n",
    "    Actions are up/down/left/right. Fires are sink states.\n",
    "\n",
    "    Living penalty is -1. Reward for rescue is +100. Penalty for fire\n",
    "    is -100 (and also fires terminate the episode.)\n",
    "    \"\"\"\n",
    "    HEIGHT, WIDTH = 4, 7\n",
    "    WALL_LOCS = [(1, c) for c in range(1, WIDTH-1)] + \\\n",
    "                [(3, c) for c in range(1, WIDTH-1)]\n",
    "    FIRE_LEFT_LOC = (2, 1)\n",
    "    FIRE_RIGHT_LOC = (2, WIDTH-2)\n",
    "    PERSON_LOCS = [(2, 2), (2, 3), (2, 4)]\n",
    "    RESCUE_REWARD = 100\n",
    "    FIRE_REWARD = -100\n",
    "    LIVING_REWARD = -1\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        state = dict(state)\n",
    "        # Get the next robot location\n",
    "        r, c = state['robot_loc']\n",
    "        dr, dc = {\n",
    "            self.UP : (-1, 0),\n",
    "            self.DOWN : (1, 0),\n",
    "            self.LEFT : (0, -1),\n",
    "            self.RIGHT : (0, 1),\n",
    "        }[action]\n",
    "        if self._allowed_robot_loc(r + dr, c + dc):\n",
    "            next_robot_loc = (r + dr, c + dc)\n",
    "        else:\n",
    "            next_robot_loc = (r, c)\n",
    "        next_robot_loc_distribution = { next_robot_loc : 1.0 }\n",
    "        # Get the next person locations\n",
    "        r, c = state['person_loc']\n",
    "        next_person_loc_distribution = { (r, c) : 0.75 }\n",
    "        adjacent_locs = []\n",
    "        if (r, c-1) in self.PERSON_LOCS:\n",
    "            adjacent_locs.append((r, c-1))\n",
    "        if (r, c+1) in self.PERSON_LOCS:\n",
    "            adjacent_locs.append((r, c+1))\n",
    "        assert len(adjacent_locs) > 0\n",
    "        for loc in adjacent_locs:\n",
    "            next_person_loc_distribution[loc] = 0.25/len(adjacent_locs)\n",
    "        # Get the next fire probs\n",
    "        next_fire_left_distribution = { \n",
    "            state['fire_left_on'] : 0.95,\n",
    "            not state['fire_left_on'] : 0.05,\n",
    "        }\n",
    "        next_fire_right_distribution = { \n",
    "            state['fire_right_on'] : 0.95,\n",
    "            not state['fire_right_on'] : 0.05,\n",
    "        }\n",
    "        # Combine factored distributions\n",
    "        return combine_factored_distributions({\n",
    "            'robot_loc' : next_robot_loc_distribution,\n",
    "            'person_loc' : next_person_loc_distribution,\n",
    "            'fire_left_on' : next_fire_left_distribution,\n",
    "            'fire_right_on' : next_fire_right_distribution,\n",
    "        })\n",
    "\n",
    "    def _loc_on_fire(self, loc, state):\n",
    "        if state['fire_left_on'] and loc == self.FIRE_LEFT_LOC:\n",
    "            return True\n",
    "        if state['fire_right_on'] and loc == self.FIRE_RIGHT_LOC:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_initial_states(self):\n",
    "        return [frozenset([\n",
    "            ('robot_loc', (0, 3)),\n",
    "            ('person_loc', (2, 3)),\n",
    "            ('fire_left_on', True),\n",
    "            ('fire_right_on', True),\n",
    "        ])]\n",
    "\n",
    "\n",
    "class SARStochasticMovementMDP(SARMDP):\n",
    "    \"\"\"An MDP for search and rescue, with stochastic action effects.\n",
    "\n",
    "    The layout is:\n",
    "\n",
    "        OFFFO\n",
    "        ROOOP\n",
    "        OWWWO\n",
    "        OWWWO\n",
    "        OWWWO\n",
    "        OOOOO\n",
    "\n",
    "    where O is empty space, W is wall, F is fire, P is person,\n",
    "    and R is the robot.\n",
    "\n",
    "    An action has the intended effect with 0.9 probability. With\n",
    "    0.1 probability, a random different action is taken.\n",
    "\n",
    "    Actions are up/down/left/right. Fires are sink states.\n",
    "\n",
    "    Reward for rescue is +100. Penalty for fire\n",
    "    is -100 (and also fires terminate the episode.)\n",
    "\n",
    "    The living penalty depends on the subclass.\n",
    "    \"\"\"\n",
    "    HEIGHT, WIDTH = 6, 5\n",
    "    WALL_LOCS = [(2, c) for c in range(1, WIDTH-1)] + \\\n",
    "                [(3, c) for c in range(1, WIDTH-1)] + \\\n",
    "                [(4, c) for c in range(1, WIDTH-1)]\n",
    "    FIRE_LOCS = [(0, c) for c in range(1, WIDTH-1)]\n",
    "    PERSON_LOC = (1, WIDTH-1)\n",
    "    RESCUE_REWARD = 100\n",
    "    FIRE_REWARD = -100\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def LIVING_REWARD(self):\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        state = dict(state)\n",
    "        # Get the next robot location\n",
    "        r, c = state['robot_loc']\n",
    "        intended_loc = None\n",
    "        unintended_locs = []\n",
    "        for a in self.get_all_actions():\n",
    "            dr, dc = {\n",
    "                self.UP : (-1, 0),\n",
    "                self.DOWN : (1, 0),\n",
    "                self.LEFT : (0, -1),\n",
    "                self.RIGHT : (0, 1),\n",
    "            }[a]\n",
    "            if self._allowed_robot_loc(r + dr, c + dc):\n",
    "                next_robot_loc = (r + dr, c + dc)\n",
    "            else:\n",
    "                next_robot_loc = (r, c)\n",
    "            if a == action:\n",
    "                intended_loc = next_robot_loc\n",
    "            else:\n",
    "                unintended_locs.append(next_robot_loc)\n",
    "        next_robot_loc_distribution = defaultdict(float)\n",
    "        next_robot_loc_distribution[intended_loc] = 0.9\n",
    "        for loc in unintended_locs:\n",
    "            next_robot_loc_distribution[loc] += 0.1/len(unintended_locs)\n",
    "        distribution = {}\n",
    "        for robot_loc, prob in next_robot_loc_distribution.items():\n",
    "            next_state = state.copy()\n",
    "            next_state['robot_loc'] = robot_loc\n",
    "            distribution[frozenset(next_state.items())] = prob\n",
    "        return distribution\n",
    "\n",
    "    def get_initial_states(self):\n",
    "        return [frozenset([\n",
    "            ('robot_loc', (1, 0)),\n",
    "            ('person_loc', self.PERSON_LOC),\n",
    "        ])]\n",
    "\n",
    "    def _loc_on_fire(self, loc, state):\n",
    "        return loc in self.FIRE_LOCS\n",
    "\n",
    "\n",
    "class ChillSARStochasticMovementMDP(SARStochasticMovementMDP):\n",
    "\n",
    "    @property\n",
    "    def LIVING_REWARD(self):\n",
    "        return -1\n",
    "\n",
    "class UrgentSARStochasticMovementMDP(SARStochasticMovementMDP):\n",
    "\n",
    "    @property\n",
    "    def LIVING_REWARD(self):\n",
    "        return -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rental Car MDP\n",
    "\n",
    "This one is for you to complete! But we recommend finishing the rest of the assignment first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RentalCarMDP(MDP):\n",
    "    \"\"\"MDP for the rental car problem that you formulated in\n",
    "    the written section.\n",
    "    \"\"\"\n",
    "    TOTAL_NUM_CARS = 3\n",
    "    MAX_DEMAND = 2\n",
    "    CITY1_RENTAL_PROBS = [0.2, 0.5, 0.3] # probability that each number\n",
    "                                         # of cars is demanded in city 1\n",
    "    CITY2_RENTAL_PROBS = [0.2, 0.4, 0.4] # probability that each number\n",
    "                                         # of cars is demanded in city 2\n",
    "    CITY1_TO_CITY2_PROB = 0.10 # the probability that a person renting a car\n",
    "                               # in city 1 wants to drive to city 2\n",
    "    CITY2_TO_CITY1_PROB = 0.25 # the probability that a person renting a car\n",
    "                               # in city 2 wants to drive to city 1\n",
    "    RENTAL_REWARD = 1.\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\"Return a list of all possible states of the environment.\n",
    "\n",
    "        We're restricted to small environments because we need\n",
    "        this function.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states : [ hashable ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def get_all_actions(self):\n",
    "        \"\"\"Return a list of all possible actions of the environment.\n",
    "\n",
    "        For simplicity, assume all actions are applicable in all states.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        actions : [ hashable ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        \"\"\"Return a dictionary of next_states to probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        next_states : { hashable : float }\n",
    "            Maps next state to prob. Sums to 1.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action in state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "            Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def get_initial_states(self):\n",
    "        \"\"\"Designate certain states as initial states. Not\n",
    "        always part of the standard MDP formalism.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states : [ hashable ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "        \n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states. Not\n",
    "        always part of the standard MDP formalism.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        is_terminal : bool\n",
    "        \"\"\"\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, max_num_iterations=1000, change_threshold=1e-4,\n",
    "                    gamma=0.99, print_every=None):\n",
    "    \"\"\"Run value iteration for a certain number of iterations or until\n",
    "    the max change between iterations is below a threshold.\n",
    "\n",
    "    Gamma is the temporal discount factor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : { hashable : { hashable : float } }\n",
    "        Q[state][action] = action-value.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_from_Q(Q, rng=np.random):\n",
    "    \"\"\"Create a policy from action-values Q\n",
    "\n",
    "    If we are sure that there are no ties, then this function could be\n",
    "    one line: lambda s : max(Q[s], key=Q[s].get)\n",
    "\n",
    "    But we want to randomly sample to break ties.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : { hashable : { hashable : float } }\n",
    "        Q[state][action] = action-value.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    policy : fn: state -> action\n",
    "    \"\"\"\n",
    "    # Create exploit policy from Q\n",
    "    def policy(s):\n",
    "        best_actions = set()\n",
    "        best_action_value = -np.inf\n",
    "        for a, val in Q[s].items():\n",
    "            if val > best_action_value:\n",
    "                best_action_value = val\n",
    "                best_actions = { a }\n",
    "            elif val == best_action_value:\n",
    "                best_actions.add(a)\n",
    "        if len(best_actions) == 1:\n",
    "            return next(iter(best_actions))\n",
    "        # Break ties randomly\n",
    "        best_actions = sorted(best_actions)\n",
    "        rng.shuffle(best_actions)\n",
    "        return best_actions[0]\n",
    "    return policy\n",
    "\n",
    "\n",
    "def sample_from_dict(dict_probs, rng):\n",
    "    \"\"\"Helper for test_action_values\n",
    "    \"\"\"\n",
    "    assert abs(sum(dict_probs.values()) - 1.) < 1e-6, \\\n",
    "        \"Probabilities do not sum to 1.\"\n",
    "    choices, probs = zip(*dict_probs.items())\n",
    "    choice_idx = rng.choice(len(choices), p=probs)\n",
    "    return choices[choice_idx]\n",
    "\n",
    "def test_action_values(Q, mdp, num_trials=500, max_trial_length=100, \n",
    "                       rng=np.random, render=False, render_mode='rgb'):\n",
    "    \"\"\"Run the policy derived from Q in the given mdp for\n",
    "    a certain number of trials. Calculate the returns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returns : [ float ]\n",
    "        One per trial.\n",
    "    \"\"\"\n",
    "    # Extract things from mdp\n",
    "    initial_states = mdp.get_initial_states()\n",
    "    assert not any(mdp.state_is_terminal(s) for s in initial_states), \\\n",
    "        \"Cannot have overlap between initial and terminal states!\"\n",
    "    T = mdp.get_transition_probabilities\n",
    "    R = mdp.get_reward\n",
    "\n",
    "    # Create policy\n",
    "    policy = create_policy_from_Q(Q, rng=rng)\n",
    "\n",
    "    # Start the evaluation\n",
    "    returns = []\n",
    "    images = []\n",
    "    for _ in range(num_trials):\n",
    "        # Sample an initial state\n",
    "        state = initial_states[rng.choice(len(initial_states))]\n",
    "        trial_returns = 0.\n",
    "        if render:\n",
    "            images.append(mdp.render(state, mode=render_mode))\n",
    "        for _ in range(max_trial_length):\n",
    "            # Get action from policy\n",
    "            action = policy(state)\n",
    "            # Take action\n",
    "            next_state = sample_from_dict(T(state, action), rng)\n",
    "            if render:\n",
    "                images.append(mdp.render(next_state, mode=render_mode))\n",
    "            # Get reward\n",
    "            reward = R(state, action, next_state)\n",
    "            trial_returns += reward\n",
    "            # Check if done\n",
    "            if mdp.state_is_terminal(next_state):\n",
    "                break\n",
    "            # Update state\n",
    "            state = next_state\n",
    "        returns.append(trial_returns)\n",
    "\n",
    "    if render:\n",
    "        return returns, images\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering (for development/debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    import cv2\n",
    "    import IPython\n",
    "    img = cv2.cvtColor(img , cv2.COLOR_RGB2BGR)\n",
    "    _, ret = cv2.imencode('.jpg', img)\n",
    "    i = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(i)\n",
    "\n",
    "def run_demo_with_rendering(mdp, max_num_iterations=1000, print_every=10, render_mode='string',\n",
    "                            seed=8):\n",
    "    \"\"\"Render mode options include \"string\" and \"rgb\"\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    Q = value_iteration(mdp, max_num_iterations=max_num_iterations, print_every=print_every)\n",
    "    returns, render_out = test_action_values(Q, mdp, num_trials=1, rng=rng,\n",
    "        render=True, render_mode=render_mode)\n",
    "    if render_mode == 'string':\n",
    "        for s in render_out:\n",
    "            print(s)\n",
    "            print()\n",
    "    else:\n",
    "        for img in render_out:\n",
    "            imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render mode options include \"string\" and \"rgb\"\n",
    "# run_demo_with_rendering(SARStochasticFiresMDP(), render_mode='string')\n",
    "# run_demo_with_rendering(ChillSARStochasticMovementMDP(), render_mode='string')\n",
    "# run_demo_with_rendering(UrgentSARStochasticMovementMDP(), render_mode='string')\n",
    "run_demo_with_rendering(SARStochasticFiresMDP(), render_mode='rgb')\n",
    "run_demo_with_rendering(ChillSARStochasticMovementMDP(), render_mode='rgb')\n",
    "run_demo_with_rendering(UrgentSARStochasticMovementMDP(), render_mode='rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    seed = 0\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # Create MDPs\n",
    "    mdps = {\n",
    "        'Debug1DGrid' : Debug1DGridMDP(),\n",
    "        'StochasticFires' : SARStochasticFiresMDP(),\n",
    "        'ChillStochasticMovement' : ChillSARStochasticMovementMDP(),\n",
    "        'UrgentStochasticMovement' : UrgentSARStochasticMovementMDP(),\n",
    "        'RentalCar' : RentalCarMDP()\n",
    "    }\n",
    "    # Create action value estimators\n",
    "    action_value_estimators = {\n",
    "        'VI-0' : lambda mdp : value_iteration(mdp, max_num_iterations=0),\n",
    "        'VI-1' : lambda mdp : value_iteration(mdp, max_num_iterations=1),\n",
    "        'VI-10' : lambda mdp : value_iteration(mdp, max_num_iterations=10),\n",
    "        'VI-100' : lambda mdp : value_iteration(mdp, max_num_iterations=100),\n",
    "        'VI-1000' : lambda mdp : value_iteration(mdp, max_num_iterations=1000, print_every=10),\n",
    "    }\n",
    "    # Evaluate each estimator in each MDP\n",
    "    all_returns = {}\n",
    "    for mdp_name, mdp in mdps.items():\n",
    "        all_returns[mdp_name] = {}\n",
    "        for estimator_name, estimator in action_value_estimators.items():\n",
    "            Q = estimator(mdp) # Run estimation! Main function\n",
    "            returns = test_action_values(Q, mdp, rng=rng)\n",
    "            all_returns[mdp_name][estimator_name] = returns\n",
    "    # Tabulate and print results\n",
    "    for mdp_name in sorted(all_returns):\n",
    "        print(f\"## {mdp_name} ##\")\n",
    "        headers = [\"Estimator\", \"Mean Returns\"]\n",
    "        table = [(estimator_name, np.mean(all_returns[mdp_name][estimator_name])) \\\n",
    "                 for estimator_name in all_returns[mdp_name]]\n",
    "        print(tabulate(table, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
