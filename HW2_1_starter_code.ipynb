{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.882 HW 2.1 Starter Code\n",
    "\n",
    "See the problem set handout for instructions and deliverables.\n",
    "\n",
    "See HW1.1 Starter Code for dependency installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run this once ever 12 hours)\n",
    "!pip install --upgrade git+https://github.com/tomsilver/pddlgym # Install most recent PDDLGym (must be from source!)\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import pddlgym\n",
    "import heapq as hq\n",
    "import numpy as np\n",
    "import time\n",
    "from itertools import count\n",
    "from collections import defaultdict, namedtuple\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Generic Approach for PO Environments\n",
    "\n",
    "An agent maintains a belief state and produces actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialObservabilityApproach:\n",
    "    \"\"\"An agent that maintains a belief state (set of possible states)\n",
    "    as it takes actions and receives partial observations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actions : [ int ]\n",
    "        A list of actions that the agent can take. All actions are\n",
    "        applicable in all states.\n",
    "    successor_fn : state, action -> state\n",
    "        Maps an environment state and action to a next state.\n",
    "    check_goal_fn : state -> bool\n",
    "        Maps an environment state to true when the goal is reached.\n",
    "    observation_fn : state -> observation\n",
    "        Maps an environment state to an observation. Sometimes\n",
    "        called \"Percept\".\n",
    "    observation_to_states_fn : observation -> frozenset{states}\n",
    "        Maps an observation to the set of environment states such\n",
    "        that observation_fn(state) would produce that observation.\n",
    "    \"\"\"\n",
    "    def __init__(self, actions, successor_fn, check_goal_fn, \n",
    "                 observation_fn, observation_to_states_fn):\n",
    "        self._actions = actions\n",
    "        self._successor_fn = successor_fn\n",
    "        self._check_goal_fn = check_goal_fn\n",
    "        self._observation_fn = observation_fn\n",
    "        self._observation_to_states_fn = observation_to_states_fn\n",
    "        self._step_count = 0\n",
    "        self._belief_state = None # set after reset\n",
    "        self._rng = None\n",
    "\n",
    "    def reset(self, obs):\n",
    "        \"\"\"Tell the agent that we have started a new problem with\n",
    "        initial observation \"obs\".\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : hashable\n",
    "            The initial observation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        info : dict\n",
    "            Any useful debug info.\n",
    "        \"\"\"\n",
    "        # Reset the belief state\n",
    "        self._belief_state = self._observation_to_states_fn(obs)\n",
    "        # Reset step count\n",
    "        self._step_count = 0\n",
    "        return {}\n",
    "\n",
    "    def step(self, obs):\n",
    "        \"\"\"Receive an observation and produce an action to be\n",
    "        immediately executed in the environment.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : hashable\n",
    "            The observation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The action is assumed to be immediately taken.\n",
    "        info : dict\n",
    "            Any useful debug info.\n",
    "        \"\"\"\n",
    "        # Update the belief state based on the observation\n",
    "        possible_states = self._observation_to_states_fn(obs)\n",
    "        # This is set intersection\n",
    "        self._belief_state &= possible_states\n",
    "        # Find an action\n",
    "        action, info = self._get_action()\n",
    "        # Update step count\n",
    "        self._step_count += 1\n",
    "        # Update the belief state based on action\n",
    "        self._belief_state = self._predict_belief_state(self._belief_state, \n",
    "            action)\n",
    "        return action, info\n",
    "\n",
    "    def seed(self, seed):\n",
    "        \"\"\"Seed the agent, just in case it's random\n",
    "        \"\"\"\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _get_action(self):\n",
    "        \"\"\"Return an action to be immediately taken, based on the current\n",
    "        belief state (self._belief_state). This is the main thing that\n",
    "        differentiates subclasses.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The action to be taken immediately.\n",
    "        info : dict\n",
    "            Any useful debug info.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def _check_belief_state_goal(self, belief_state):\n",
    "        \"\"\"Check whether the belief state is a goal, that is, whether\n",
    "        all states in the belief state satisify the check_goal_fn.\n",
    "\n",
    "        This function is included here because it is likely to be\n",
    "        used by subclasses.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        belief_state : frozenset{hashable}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        goal_reached : bool\n",
    "        \"\"\"\n",
    "        # We've found a goal if all states in the belief state are at goals\n",
    "        for state in belief_state:\n",
    "            if not self._check_goal_fn(state):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _predict_belief_state(self, belief_state, action):\n",
    "        \"\"\"Get the next belief state that would result after taking\n",
    "        action in belief_state.\n",
    "\n",
    "        This function is included here because it is likely to be\n",
    "        used by subclasses.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        belief_state : frozenset{hashable}\n",
    "        action : int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        next_belief_state : frozenset{hashable}\n",
    "        \"\"\"\n",
    "        next_belief_state = set()\n",
    "        for state in belief_state:\n",
    "            next_state = self._successor_fn(state, action)\n",
    "            next_belief_state.add(next_state)\n",
    "        return frozenset(next_belief_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Actions Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomActions(PartialObservabilityApproach):\n",
    "    \"\"\"Take random actions\n",
    "    \"\"\"\n",
    "    def _get_action(self):\n",
    "        return self._rng.choice(self._actions), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth-first And-Or Search\n",
    "Finish implementing this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AndOrSearch(PartialObservabilityApproach):\n",
    "    \"\"\"Exhaustive depth-first And-Or search\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._conditional_plan = \"failure\"\n",
    "\n",
    "    def _get_action(self):\n",
    "        \"\"\"Return an action to be immediately taken, based on the current\n",
    "        belief state (self._belief_state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The action to be taken immediately.\n",
    "        info : dict\n",
    "            Any useful debug info.\n",
    "        \"\"\"\n",
    "        # If self._step_count == 0, get a new plan from self._get_conditional_plan()\n",
    "        # Execute the next action in the conditional plan\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def _get_conditional_plan(self):\n",
    "        \"\"\"Run planning from scratch, given the current belief state\n",
    "        \"\"\"\n",
    "        # Start off the AND-OR search\n",
    "        return self._run_or_search(self._belief_state, [])\n",
    "\n",
    "    def _run_or_search(self, belief_state, path, depth=0, max_depth=np.inf):\n",
    "        \"\"\"Run OR part of AO search (recursively).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        belief_state : frozenset{hashable}\n",
    "        path : [ belief_state ]\n",
    "            Belief states encountered so far, used to find cycles.\n",
    "        depth : int\n",
    "        max_depth : int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        conditional_plan : Any\n",
    "            Representation of the conditional plan.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def _run_and_search(self, belief_states, path, depth=0, max_depth=np.inf):\n",
    "        \"\"\"Run AND part of the AO search (recursively).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        belief_states : [frozenset{hashabale}]\n",
    "            A list of belief states.\n",
    "        path : [ belief_state ]\n",
    "            Belief states encountered so far, used to find cycles.\n",
    "        depth : int\n",
    "        max_depth : int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        conditional_plan : Any\n",
    "            Representation of the conditional plan.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Deepening And-Or Search\n",
    "No need to modify this (but you can if you want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeDeepeningAndOrSearch(AndOrSearch):\n",
    "    \"\"\"Run AndOrSearch with progressively larger depth limits until a plan is found.\n",
    "    \"\"\"\n",
    "    def _get_conditional_plan(self):\n",
    "        \"\"\"Run planning from scratch, given the current belief state\n",
    "        \"\"\"\n",
    "        # Run iterative deepening planning until plan is not a failure\n",
    "        for max_depth in count(1):\n",
    "            print(f\"Running iterative deepening with depth {max_depth}\", end='\\r', flush=True)\n",
    "            conditional_plan = self._run_or_search(self._belief_state, [], \n",
    "                depth=0, max_depth=max_depth)\n",
    "            if conditional_plan != \"failure\":\n",
    "                print()\n",
    "                break\n",
    "        return conditional_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AStar Planner\n",
    "Used by Single State Determinization. No need to modify this (but you can if you want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AStar:\n",
    "    \"\"\"Planning with A* search. Used by SingleStateDeterminization.\n",
    "    \"\"\"\n",
    "    \n",
    "    Node = namedtuple(\"Node\", [\"state\", \"parent\", \"action\", \"g\"])\n",
    "\n",
    "    def __init__(self, successor_fn, check_goal_fn, heuristic=None, timeout=100):\n",
    "        self._get_successor_state = successor_fn\n",
    "        self._check_goal = check_goal_fn\n",
    "        self._heuristic = heuristic or (lambda s : 0)\n",
    "        self._timeout = timeout\n",
    "        self._actions = None\n",
    "        \n",
    "    def __call__(self, state, verbose=True):\n",
    "        return self._get_plan(state, verbose=verbose)\n",
    "\n",
    "    def set_actions(self, actions):\n",
    "        self._actions = actions\n",
    "\n",
    "    def _get_plan(self, state, verbose=True):\n",
    "        start_time = time.time()\n",
    "        queue = []\n",
    "        state_to_best_g = defaultdict(lambda : float(\"inf\"))\n",
    "        tiebreak = count()\n",
    "\n",
    "        root_node = self.Node(state=state, parent=None, action=None, g=0)\n",
    "        hq.heappush(queue, (self._get_priority(root_node), next(tiebreak), root_node))\n",
    "        num_expansions = 0\n",
    "\n",
    "        while len(queue) > 0 and (time.time() - start_time < self._timeout):\n",
    "            _, _, node = hq.heappop(queue)\n",
    "            # If we already found a better path here, don't bother\n",
    "            if state_to_best_g[node.state] < node.g:\n",
    "                continue\n",
    "            # If the goal holds, return\n",
    "            if self._check_goal(node.state):\n",
    "                if verbose:\n",
    "                    print(\"\\nPlan found!\")\n",
    "                return self._finish_plan(node), {'node_expansions' : num_expansions}\n",
    "            num_expansions += 1\n",
    "            if verbose:\n",
    "                print(f\"Expanding node {num_expansions}\", end='\\r', flush=True)\n",
    "            # Generate successors\n",
    "            for action, child_state in self._get_successors(node.state):\n",
    "                # If we already found a better path to child, don't bother\n",
    "                if state_to_best_g[child_state] <= node.g+1:\n",
    "                    continue\n",
    "                # Add new node\n",
    "                child_node = self.Node(state=child_state, parent=node, action=action, g=node.g+1)\n",
    "                priority = self._get_priority(child_node)\n",
    "                hq.heappush(queue, (priority, next(tiebreak), child_node))\n",
    "                state_to_best_g[child_state] = child_node.g\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Warning: planning failed.\")\n",
    "        return [], {'node_expansions' : num_expansions}\n",
    "    \n",
    "    def _get_successors(self, state):\n",
    "        for action in self._actions:\n",
    "            next_state = self._get_successor_state(state, action)\n",
    "            yield action, next_state\n",
    "\n",
    "    def _finish_plan(self, node):\n",
    "        plan = []\n",
    "        while node.parent is not None:\n",
    "            plan.append(node.action)\n",
    "            node = node.parent\n",
    "        plan.reverse()\n",
    "        return plan\n",
    "\n",
    "    def _get_priority(self, node):\n",
    "        h = self._heuristic(node)\n",
    "        if isinstance(h, tuple):\n",
    "            return (tuple(node.g + hi for hi in h), h)\n",
    "        return (node.g + h, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single State Determinization\n",
    "Finish implementing this class. (This should be relatively short.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleStateDeterminization(PartialObservabilityApproach):\n",
    "    \"\"\"Arbitrarily select a state from the belief state and then\n",
    "    do uniform cost search.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._plan = []\n",
    "        self._planner = AStar(self._successor_fn, self._check_goal_fn)\n",
    "        self._planner.set_actions(self._actions)\n",
    "\n",
    "    def _get_action(self):\n",
    "        \"\"\"Return an action to be immediately taken, based on the current\n",
    "        belief state (self._belief_state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The action to be taken immediately.\n",
    "        info : dict\n",
    "            Any useful debug info.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCT Planner\n",
    "Used by PO-UCT. No need to modify this (but you can if you want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCT:\n",
    "    \"\"\"Implementation of UCT based on Leslie's lecture notes. Used by POUCT.\n",
    "    \"\"\"\n",
    "    def __init__(self, actions, reward_fn, transition_fn, done_fn=None, num_search_iters=100, gamma=0.9):\n",
    "        self._actions = actions\n",
    "        self._reward_fn = reward_fn\n",
    "        self._transition_fn = transition_fn\n",
    "        self._done_fn = done_fn or (lambda s,a : False)\n",
    "        self._num_search_iters = num_search_iters\n",
    "        self._gamma = gamma\n",
    "        self._rng = None # set in seed\n",
    "        self._Q = None\n",
    "        self._N = None\n",
    "        self._node_expansions = 0\n",
    "\n",
    "    def run(self, state, horizon=100):\n",
    "        # Initialize Q[s][a][d] -> float\n",
    "        self._Q = defaultdict(lambda : defaultdict(lambda : defaultdict(float)))\n",
    "        # Initialize N[s][a][d] -> int\n",
    "        self._N = defaultdict(lambda : defaultdict(lambda : defaultdict(int)))\n",
    "        # Loop search\n",
    "        for it in range(self._num_search_iters):\n",
    "            # Update Q\n",
    "            self._search(state, 0, horizon=horizon)\n",
    "        info = {\"node_expansions\" : self._node_expansions}\n",
    "        self._node_expansions = 0\n",
    "        return info\n",
    "\n",
    "    def get_action(self, state, t=0):\n",
    "        # Return best action, break ties randomly\n",
    "        return max(self._actions, key=lambda a : (self._Q[state][a][t], self._rng.uniform()))\n",
    "\n",
    "    def _search(self, s, depth, horizon=100):\n",
    "        # Base case\n",
    "        if depth == horizon:\n",
    "            return 0.\n",
    "        # Select an action, balancing explore/exploit\n",
    "        a = self._select_action(s, depth, horizon=horizon)\n",
    "        # Create a child state\n",
    "        next_state = self._transition_fn(s, a)\n",
    "        self._node_expansions += 1\n",
    "        # Get value estimate\n",
    "        if self._done_fn(s, a):\n",
    "            # Some environments terminate problems before the horizon \n",
    "            q = self._reward_fn(s, a)\n",
    "        else:\n",
    "            q = self._reward_fn(s, a) + self._gamma * self._search(next_state, depth+1, horizon=horizon)\n",
    "        # Update values and counts\n",
    "        num_visits = self._N[s][a][depth] # before now\n",
    "        # First visit to (s, a, depth)\n",
    "        if num_visits == 0:\n",
    "            self._Q[s][a][depth] = q\n",
    "        # We've been here before\n",
    "        else:\n",
    "            # Running average\n",
    "            self._Q[s][a][depth] = (num_visits / (num_visits + 1.)) * self._Q[s][a][depth] + \\\n",
    "                                   (1 / (num_visits + 1.)) * q\n",
    "        # Update num visits\n",
    "        self._N[s][a][depth] += 1\n",
    "        return self._Q[s][a][depth]\n",
    "\n",
    "    def _select_action(self, s, depth, horizon):\n",
    "        # If there is any action where N(s, a, depth) == 0, try it first\n",
    "        untried_actions = [a for a in self._actions if self._N[s][a][depth] == 0]\n",
    "        if len(untried_actions) > 0:\n",
    "            return self._rng.choice(untried_actions)\n",
    "        # Otherwise, take an action to trade off exploration and exploitation\n",
    "        N_s_d = sum(self._N[s][a][depth] for a in self._actions)\n",
    "        best_action_score = -np.inf\n",
    "        best_actions = []\n",
    "        for a in self._actions:\n",
    "            explore_bonus = (np.log(N_s_d) / self._N[s][a][depth])**((horizon + depth) / (2*horizon + depth))\n",
    "            score = self._Q[s][a][depth] + explore_bonus\n",
    "            if score > best_action_score:\n",
    "                best_action_score = score\n",
    "                best_actions = [a]\n",
    "            elif score == best_action_score:\n",
    "                best_actions.append(a)\n",
    "        return self._rng.choice(best_actions)\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self._rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PO-UCT\n",
    "Finish implementing this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POUCT(PartialObservabilityApproach):\n",
    "    \"\"\"Use UCT in belief space; sample belief state transitions uniformly at random\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._planner = UCT(self._actions, self._get_uct_reward, self._get_uct_transition,\n",
    "                            done_fn=lambda s,a:self._check_belief_state_goal(s),\n",
    "                            num_search_iters=100, gamma=0.9)\n",
    "        self._steps_since_replanning = 0\n",
    "        self._replanning_interval = 1\n",
    "        self._horizon = 50\n",
    "\n",
    "    def _get_action(self):\n",
    "        \"\"\"Return an action to be immediately taken, based on the current\n",
    "        belief state (self._belief_state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The action to be taken immediately.\n",
    "        info : dict\n",
    "            Any useful debug info.\n",
    "        \"\"\"\n",
    "        info = {}\n",
    "        # Replan on a fixed interval\n",
    "        if self._step_count % self._replanning_interval == 0:\n",
    "            info = self._planner.run(self._belief_state, horizon=self._horizon)\n",
    "            self._steps_since_replanning = 0\n",
    "        action = self._planner.get_action(self._belief_state, t=self._steps_since_replanning)\n",
    "        self._steps_since_replanning += 1\n",
    "        return action, info\n",
    "\n",
    "    def _get_plan(self):\n",
    "        \"\"\"Determinize and plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        plan : [ int ]\n",
    "            A sequence of actions.\n",
    "        \"\"\"\n",
    "        self._planner.run(self._belief_state)\n",
    "        return self._planner.get_action(self._belief_state)\n",
    "\n",
    "    def _get_uct_reward(self, belief_state, _):\n",
    "        \"\"\"Use a sparse reward: 1.0 if the goal is reached, 0 otherwise\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        belief_state : frozenset{hashable}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def _get_uct_transition(self, belief_state, action):\n",
    "        \"\"\"Sample uniformly at random among the possible next belief states\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def seed(self, seed):\n",
    "        \"\"\"Also seed the planner\n",
    "        \"\"\"\n",
    "        super().seed(seed)\n",
    "        self._planner.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline\n",
    "No need to modify this (but you can if you want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_test(test_env, problem_idx, model, max_horizon=100):\n",
    "    print(f\"Running test problem {problem_idx}\")\n",
    "    test_env.fix_problem_index(problem_idx)\n",
    "    start_time = time.time()\n",
    "    obs, info = test_env.reset()\n",
    "    model_info = model.reset(obs)\n",
    "    num_steps = 0\n",
    "    expansions = model_info.get(\"node_expansions\", 0)\n",
    "    success = False\n",
    "    for t in range(max_horizon):\n",
    "        print(\".\", end='', flush=True)\n",
    "        act, model_info = model.step(obs)\n",
    "        expansions += model_info.get(\"node_expansions\", 0)\n",
    "        obs, reward, done, info = test_env.step(act)\n",
    "        num_steps += 1\n",
    "        if done:\n",
    "            assert reward == 1\n",
    "            success = True\n",
    "            break\n",
    "    duration = time.time() - start_time\n",
    "    print(f\" final duration: {duration} with num steps {num_steps} and success={success}.\")\n",
    "    return duration, expansions, num_steps, success\n",
    "\n",
    "def run_single_experiment(model, env, seed=0):\n",
    "    # Initialize\n",
    "    model.seed(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Do testing\n",
    "    test_durations = [] # seconds, one per problem\n",
    "    test_expansions = [] # integers\n",
    "    test_num_steps = [] # integers\n",
    "    test_successes = [] # boolean, True if successful\n",
    "    for problem_idx in range(len(env.problems)):\n",
    "        duration, expansions, num_steps, success = \\\n",
    "            run_single_test(env, problem_idx, model)\n",
    "        test_durations.append(duration)\n",
    "        test_expansions.append(expansions)\n",
    "        test_num_steps.append(num_steps)\n",
    "        test_successes.append(success)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return test_durations, test_expansions, test_num_steps, test_successes\n",
    "\n",
    "def get_approach(name, env, planning_timeout=10):\n",
    "    if name == \"random\":\n",
    "        return RandomActions(env.get_possible_actions(), env.get_successor_state, \n",
    "                             env.check_goal, env.get_observation, env.observation_to_states)\n",
    "\n",
    "    if name == \"depth_first_and_or_search\":\n",
    "        return AndOrSearch(env.get_possible_actions(), env.get_successor_state, \n",
    "                           env.check_goal, env.get_observation, env.observation_to_states)\n",
    "\n",
    "    if name == \"iterative_deepening_and_or_search\":\n",
    "        return IterativeDeepeningAndOrSearch(env.get_possible_actions(), env.get_successor_state, \n",
    "                                             env.check_goal, env.get_observation, env.observation_to_states)\n",
    "\n",
    "    if name == \"single_state_determinization\":\n",
    "        return SingleStateDeterminization(env.get_possible_actions(), env.get_successor_state, \n",
    "                                          env.check_goal, env.get_observation, env.observation_to_states)\n",
    "\n",
    "    if name == \"pouct\":\n",
    "        return POUCT(env.get_possible_actions(), env.get_successor_state, \n",
    "                     env.check_goal, env.get_observation, env.observation_to_states)\n",
    "\n",
    "\n",
    "    raise Exception(f\"Unrecognized approach: {name}\")\n",
    "\n",
    "def print_results_table(env_name, results_for_env):\n",
    "    print(f\"\\n### {env_name} ###\")\n",
    "    mean_table = [(a, ) + tuple(np.mean(results_for_env[a], axis=0)) \\\n",
    "                  for a in sorted(results_for_env)]\n",
    "    columns = [\"Approach\", \"Duration\", \"Expansions\", \"Num Steps\", \"Successes\"]\n",
    "    print(tabulate(mean_table, headers=columns))\n",
    "\n",
    "def main():\n",
    "    approaches = [\n",
    "        \"random\", \n",
    "        \"depth_first_and_or_search\",\n",
    "        \"iterative_deepening_and_or_search\",\n",
    "        \"single_state_determinization\",\n",
    "        \"pouct\",\n",
    "    ]\n",
    "    num_seeds_per_approach = {\n",
    "        \"random\" : 10,\n",
    "        \"depth_first_and_or_search\" : 1,\n",
    "        \"iterative_deepening_and_or_search\" : 1,\n",
    "        \"single_state_determinization\" : 10,\n",
    "        \"pouct\" : 10,\n",
    "    }\n",
    "\n",
    "    env_names = [\n",
    "        \"SmallPOSARRadius0\",\n",
    "        \"POSARRadius1\",\n",
    "        \"POSARRadius0\",\n",
    "        \"POSARRadius1Xray\", \n",
    "        \"POSARRadius0Xray\",\n",
    "    ]\n",
    "\n",
    "    all_results = {}\n",
    "    for env_name in env_names:\n",
    "        results_for_env = {}\n",
    "        all_results[env_name] = results_for_env\n",
    "        for approach in approaches:\n",
    "            results_for_env[approach] = []\n",
    "            env = pddlgym.make(f\"{env_name}-v0\")\n",
    "            model = get_approach(approach, env)\n",
    "            for seed in range(num_seeds_per_approach[approach]):\n",
    "                results = run_single_experiment(model, env, seed=seed)\n",
    "                for (dur, expansions, num_steps, succ) in zip(*results):\n",
    "                    results_for_env[approach].append((dur, expansions, num_steps, succ))\n",
    "\n",
    "        # Print per-environment results (because of impatience)\n",
    "        print_results_table(env_name, results_for_env)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\n\" + \"*\" * 80)\n",
    "    for env_name in env_names:\n",
    "        print_results_table(env_name, all_results[env_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire Away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
